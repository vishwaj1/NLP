{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishwaj1/NLP/blob/main/GA_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMFr0ImTfbkV",
        "outputId": "feed0307-7b4e-4449-9dd5-c9daa24679e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PbsrmACwenTb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from conllu import parse_incr\n",
        "\n",
        "def load_data(file_path):\n",
        "    sentences = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for tokenlist in parse_incr(file):\n",
        "            words = [token['form'] for token in tokenlist]\n",
        "            pos_tags = [token['upostag'] for token in tokenlist]\n",
        "            heads = [token['head'] for token in tokenlist]\n",
        "            labels = [token['deprel'] for token in tokenlist]\n",
        "            sentences.append((words, pos_tags, heads, labels))\n",
        "    return sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K00kesKhgoT-"
      },
      "outputs": [],
      "source": [
        "def build_vocab(data):\n",
        "    words = set()\n",
        "    pos_tags = set()\n",
        "    labels = set()\n",
        "    for sentence in data:\n",
        "        words.update(sentence[0])\n",
        "        pos_tags.update(sentence[1])\n",
        "        labels.update(sentence[3])\n",
        "    word2idx = {word: i + 1 for i, word in enumerate(words)}  # +1 to start index from 1\n",
        "    word2idx['<UNK>'] = 0  # Unknown words\n",
        "    pos2idx = {tag: i + 1 for i, tag in enumerate(pos_tags)}  # +1 to start index from 1\n",
        "    pos2idx['<UNK>'] = 0  # Unknown POS tags\n",
        "    label2idx = {label: i for i, label in enumerate(labels)}\n",
        "    return word2idx, pos2idx, label2idx\n",
        "\n",
        "# Load data\n",
        "train_data = load_data('train.gold.conll')\n",
        "\n",
        "# Build vocabularies\n",
        "word2idx, pos2idx, label2idx = build_vocab(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4ZmMy_0_euvu"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class DependencyParsingDataset(Dataset):\n",
        "    def __init__(self, data, word2idx, pos2idx, label2idx):\n",
        "        self.data = data\n",
        "        self.word2idx = word2idx\n",
        "        self.pos2idx = pos2idx\n",
        "        self.label2idx = label2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, pos_tags, heads, labels = self.data[idx]\n",
        "        word_idxs = [self.word2idx.get(word.lower(), self.word2idx['<UNK>']) for word in words]\n",
        "        pos_idxs = [self.pos2idx.get(pos, self.pos2idx['<UNK>']) for pos in pos_tags]\n",
        "        head_idxs = [int(head) for head in heads]  # Ensure head indices are integers\n",
        "        label_idxs = [self.label2idx[label] for label in labels]\n",
        "        return torch.tensor(word_idxs, dtype=torch.long), \\\n",
        "               torch.tensor(pos_idxs, dtype=torch.long), \\\n",
        "               torch.tensor(head_idxs, dtype=torch.long), \\\n",
        "               torch.tensor(label_idxs, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    word_idxs, pos_idxs, head_idxs, label_idxs = zip(*batch)\n",
        "    word_idxs = pad_sequence(word_idxs, batch_first=True, padding_value=0)\n",
        "    pos_idxs = pad_sequence(pos_idxs, batch_first=True, padding_value=0)\n",
        "    head_idxs = pad_sequence(head_idxs, batch_first=True, padding_value=0)\n",
        "    label_idxs = pad_sequence(label_idxs, batch_first=True, padding_value=0)\n",
        "    return word_idxs, pos_idxs, head_idxs, label_idxs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7DUtW5dQxQmq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "class DependencyParser(nn.Module):\n",
        "    def __init__(self, vocab_size, pos_size, num_labels, embedding_dim, hidden_dim):\n",
        "        super(DependencyParser, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_embeddings = nn.Embedding(pos_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, num_labels)\n",
        "\n",
        "    def forward(self, word_idxs, pos_idxs):\n",
        "        # Ensure that word_idxs and pos_idxs are of the shape [batch_size, sequence_length]\n",
        "        word_embeds = self.word_embeddings(word_idxs)  # [batch_size, seq_length, embedding_dim]\n",
        "        pos_embeds = self.pos_embeddings(pos_idxs)     # [batch_size, seq_length, embedding_dim]\n",
        "\n",
        "        # Concatenation should work as both have three dimensions\n",
        "        embeds = torch.cat((word_embeds, pos_embeds), dim=2)  # Concatenate along the last dimension\n",
        "\n",
        "        lstm_out, _ = self.lstm(embeds)  # LSTM expects input of shape [batch_size, seq_length, features]\n",
        "        label_space = self.hidden2label(lstm_out)\n",
        "        return label_space\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXNx6MYzew8p",
        "outputId": "b6f805af-cc48-424c-f3c0-72ccdf16a6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25191973025899334\n",
            "Epoch 1, Loss: 0.18227184348436723\n",
            "Epoch 2, Loss: 0.16593886163220348\n",
            "Epoch 3, Loss: 0.1564775746809431\n",
            "Epoch 4, Loss: 0.14959591199236222\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_model(model, dataset, epochs, learning_rate):\n",
        "    data_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=0)  # Assuming '0' is used for padding\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for word_idxs, pos_idxs, head_idxs, label_idxs in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(word_idxs, pos_idxs)\n",
        "            loss = loss_function(outputs.view(-1, outputs.shape[-1]), label_idxs.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(data_loader)}\")\n",
        "\n",
        "\n",
        "\n",
        "dataset = DependencyParsingDataset(train_data, word2idx, pos2idx, label2idx)\n",
        "model = DependencyParser(len(word2idx), len(pos2idx), len(label2idx), 100, 200)\n",
        "train_model(model, dataset, 5, 0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bOMfTqoKw-FZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b50a178-fb0f-4f50-c7d8-c17c8953dd4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5NIZh-Jxnd87"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate(model, dataset, device=torch.device('cpu')):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    model.to(device)\n",
        "\n",
        "    all_true_heads = []\n",
        "    all_pred_heads = []\n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for word_idxs, pos_idxs, head_idxs, label_idxs in DataLoader(dataset, batch_size=1, collate_fn=collate_fn):\n",
        "            word_idxs, pos_idxs, head_idxs, label_idxs = word_idxs.to(device), pos_idxs.to(device), head_idxs.to(device), label_idxs.to(device)\n",
        "            outputs = model(word_idxs, pos_idxs)\n",
        "            _, predicted_heads = torch.max(outputs, dim=2)\n",
        "            predicted_labels = predicted_heads\n",
        "            # Flatten the tensors for metric calculation\n",
        "            all_true_heads.extend(head_idxs.view(-1).cpu().numpy())\n",
        "            all_pred_heads.extend(predicted_heads.view(-1).cpu().numpy())\n",
        "            all_true_labels.extend(label_idxs.view(-1).cpu().numpy())\n",
        "            all_pred_labels.extend(predicted_labels.view(-1).cpu().numpy())\n",
        "\n",
        "    uas = accuracy_score(all_true_heads, all_pred_heads)\n",
        "    las = accuracy_score(all_true_labels, all_pred_labels)\n",
        "\n",
        "\n",
        "    return uas, las\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jOLCVPxWne3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743e167d-1cb9-42e5-c575-a1b826897a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for test data\n",
            "UAS: 2.54%\n",
            "LAS: 2.13%\n",
            "\n",
            "Results for dev data\n",
            "UAS: 2.54%\n",
            "LAS: 2.08%\n"
          ]
        }
      ],
      "source": [
        "test_data = load_data('test.gold.conll')\n",
        "test_dataset = DependencyParsingDataset(test_data, word2idx, pos2idx, label2idx)\n",
        "\n",
        "print(\"Results for test data\")\n",
        "uas, las = evaluate(model, test_dataset)\n",
        "print(f\"UAS: {uas*100:.2f}%\")\n",
        "print(f\"LAS: {las*100:.2f}%\")\n",
        "\n",
        "\n",
        "print(\"\\nResults for dev data\")\n",
        "dev_data = load_data('dev.gold.conll')\n",
        "dev_dataset = DependencyParsingDataset(dev_data, word2idx, pos2idx, label2idx)\n",
        "uas, las = evaluate(model, dev_dataset)\n",
        "print(f\"UAS: {uas*100:.2f}%\")\n",
        "print(f\"LAS: {las*100:.2f}%\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1m-IPv7UsKf"
      },
      "source": [
        "# German Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "leqn2WWnUyIx"
      },
      "outputs": [],
      "source": [
        "def load_data_g(file_path):\n",
        "    sentences = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for tokenlist in parse_incr(file):\n",
        "            if any(token['head'] is None for token in tokenlist):  # Check if any head is None\n",
        "                continue  # Skip this sentence or handle it differently\n",
        "            words = [token['form'] for token in tokenlist]\n",
        "            pos_tags = [token['upostag'] for token in tokenlist]\n",
        "            heads = [token['head'] for token in tokenlist if token['head'] is not None]\n",
        "            labels = [token['deprel'] for token in tokenlist]\n",
        "            sentences.append((words, pos_tags, heads, labels))\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QkXi7D64VAPj"
      },
      "outputs": [],
      "source": [
        "def build_vocab_g(data):\n",
        "    words = set()\n",
        "    pos_tags = set()\n",
        "    labels = set()\n",
        "    for sentence in data:\n",
        "        words.update(sentence[0])\n",
        "        pos_tags.update(sentence[1])\n",
        "        labels.update(sentence[3])\n",
        "    word2idx = {word: i + 1 for i, word in enumerate(words)}  # +1 to start index from 1\n",
        "    word2idx['<UNK>'] = 0  # Unknown words\n",
        "    pos2idx = {tag: i + 1 for i, tag in enumerate(pos_tags)}  # +1 to start index from 1\n",
        "    pos2idx['<UNK>'] = 0  # Unknown POS tags\n",
        "    label2idx = {label: i for i, label in enumerate(labels)}\n",
        "    return word2idx, pos2idx, label2idx\n",
        "\n",
        "# Load data\n",
        "#dev_data = load_data('dev.gold.conll')\n",
        "train_data = load_data_g('de_gsd-ud-train.conllu')\n",
        "\n",
        "# Build vocabularies\n",
        "word2idx, pos2idx, label2idx = build_vocab_g(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9y9qsAdtU1Rt"
      },
      "outputs": [],
      "source": [
        "class DependencyParsingDataset_g(Dataset):\n",
        "    def __init__(self, data, word2idx, pos2idx, label2idx):\n",
        "        self.data = data\n",
        "        self.word2idx = word2idx\n",
        "        self.pos2idx = pos2idx\n",
        "        self.label2idx = label2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, pos_tags, heads, labels = self.data[idx]\n",
        "        word_idxs = [self.word2idx.get(word.lower(), self.word2idx['<UNK>']) for word in words]\n",
        "        pos_idxs = [self.pos2idx.get(pos, self.pos2idx['<UNK>']) for pos in pos_tags]\n",
        "        head_idxs = [int(head) if head is not None else -1 for head in heads]  # Replace None with -1\n",
        "        label_idxs = [self.label2idx[label] for label in labels]\n",
        "        return torch.tensor(word_idxs, dtype=torch.long), \\\n",
        "               torch.tensor(pos_idxs, dtype=torch.long), \\\n",
        "               torch.tensor(head_idxs, dtype=torch.long), \\\n",
        "               torch.tensor(label_idxs, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6UzJB_tLU3To",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba8fe62-e1cf-4e09-de93-abe1ad3770e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2211680657333798\n",
            "Epoch 1, Loss: 0.1293135745066499\n",
            "Epoch 2, Loss: 0.10583970916886178\n",
            "Epoch 3, Loss: 0.09089360660503781\n",
            "Epoch 4, Loss: 0.07876359896054344\n"
          ]
        }
      ],
      "source": [
        "def train_model_g(model, dataset, epochs, learning_rate):\n",
        "    data_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=-1)  # Assuming -1 is used for missing values\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for word_idxs, pos_idxs, head_idxs, label_idxs in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(word_idxs, pos_idxs)\n",
        "            loss = loss_function(outputs.view(-1, outputs.shape[-1]), label_idxs.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(data_loader)}\")\n",
        "\n",
        "dataset = DependencyParsingDataset_g(train_data, word2idx, pos2idx, label2idx)\n",
        "model = DependencyParser(len(word2idx), len(pos2idx), len(label2idx), 100, 200)\n",
        "train_model_g(model, dataset, 5, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7YyX5HY_W1q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d00bd77-5494-468e-bb82-18464310a8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for test data\n",
            "UAS: 2.57%\n",
            "LAS: 2.11%\n",
            "\n",
            "Results for dev data\n",
            "UAS: 2.28%\n",
            "LAS: 1.97%\n"
          ]
        }
      ],
      "source": [
        "test_data = load_data_g('de_gsd-ud-test.conllu')\n",
        "test_dataset = DependencyParsingDataset(test_data, word2idx, pos2idx, label2idx)\n",
        "\n",
        "print(\"Results for test data\")\n",
        "uas, las = evaluate(model, test_dataset)\n",
        "print(f\"UAS: {uas*100:.2f}%\")\n",
        "print(f\"LAS: {las*100:.2f}%\")\n",
        "\n",
        "print(\"\\nResults for dev data\")\n",
        "dev_data = load_data_g('de_gsd-ud-dev.conllu')\n",
        "dev_dataset = DependencyParsingDataset(dev_data, word2idx, pos2idx, label2idx)\n",
        "uas, las = evaluate(model, dev_dataset)\n",
        "print(f\"UAS: {uas*100:.2f}%\")\n",
        "print(f\"LAS: {las*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CeH7c5UWdYzg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNQVvCxPbQn48sLhwJ3Z1Iw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}